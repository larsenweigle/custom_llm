{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning LLaMa v2 with QLoRA\n",
        "\n",
        "This [documentation](https://rentry.org/llm-training) and [blog post](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32) were extremely helpful for the creation of the notebook. Most of the code is adapted from the blog post linked above, however, I try to add more explanation in the hope of enhancing my own understanding. I modified the dataset curation process as well."
      ],
      "metadata": {
        "id": "5qzIMB3XMrv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Libraries"
      ],
      "metadata": {
        "id": "AVlrdPuY_SpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following libraries are installed:\n",
        "- [Accelerate](https://huggingface.co/docs/accelerate/index): This enables PyTorch code to runs across differnet configurations. Using Accelerate, we can dynamically configure how models consume different resources (GPU, CPU, etc).\n",
        "- [PEFT](https://huggingface.co/docs/peft/index): Library that implements various parameter efficient fine-tuning methods for language models (LoRA, Prefix Tuning, Prompt Tuning, etc).\n",
        "- [bitsandbytes](https://pypi.org/project/bitsandbytes/): In combination with the transformers library, bitsandbytes enables models to be loaded in 8-bit precision (and more recently 4-bit!), which is critical for reducing memory usage when fine-tuning LLMs.\n",
        "- [Transformers](https://huggingface.co/docs/transformers/index): Provides the basis for all tools and models related to the [transformer architecture](https://arxiv.org/abs/1706.03762) on Hugging Face.\n",
        "- [TRL](https://pypi.org/project/trl/): the Transformer Reinforcement Learning Library provides a set of tools for the various steps of RLHF. In particular, this tutorial uses SFTTrainer to implement supervised fine-tuning for LLaMa v2.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ib7bLvnkg_Qn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzETOm96_Iaj"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes transformers==4.31.0 trl==0.4.7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's log in to the Hugging Face CLI too."
      ],
      "metadata": {
        "id": "BPdsQOCczCl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "rnMJ3GoFyhut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this next block is not necessary to fine tune the model, I think it is important to get in the habit of tracking and reporting Carbon Dioxoide emmissions on model cards ðŸŒ±\n",
        "\n",
        "We will use the library later to track emissions from our fine tuning step."
      ],
      "metadata": {
        "id": "tPzTrielKDqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install codecarbon"
      ],
      "metadata": {
        "id": "jVnmWlEUKg_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up packages"
      ],
      "metadata": {
        "id": "pyGXELcm_hip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "CkeBXWfV_h0n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model and Dataset\n",
        "\n",
        "First, the model and data names are instantiated. These will be used later when downloading a pretrained model and dataset from Hugging Face using `AutoModelForCausalLM` and `AutoTokenizer`. Here the new model name is specified as well.  "
      ],
      "metadata": {
        "id": "NoIf71ud_rfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"llama-2-7b-miniguanaco\""
      ],
      "metadata": {
        "id": "o2UrMRZ__sCV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Configuration âœ¨\n",
        "\n",
        "This is where the magic happens to create different specialized versions of LLMs. In the case that the dataset is not preprocessed, it is necessary to process it here in a way that LLaMa v2 expects (ex: f\"\\<s>[INST] {prompt} [/INST]\")."
      ],
      "metadata": {
        "id": "MciIsN81RIUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (you can process it here)\n",
        "dataset = load_dataset(dataset_name, split=\"train\")"
      ],
      "metadata": {
        "id": "cOeQCNjERHvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bitsandbytes Configuration"
      ],
      "metadata": {
        "id": "8QH-J_C7Rh-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The section below sets up the arguments for the `BitsAndBytesConfig` instance. Using the 4-bit configuration, quantized versions of compatible Hugging Face models can be downloaded to the model. This significantly reduces the memory foot print of LLaMa v2!\n",
        "\n",
        "In particular, the \"nf4\" (Normal Float 4) datatype is a 4-bit datatype that has been designed for variables that have been sampled from a normal distribution. This is perfect for normalized weights in transformers. If you are curious on what exactly is going on under the hood, refer to this [blog](https://huggingface.co/blog/hf-bitsandbytes-integration) and this [post](https://huggingface.co/blog/4bit-transformers-bitsandbytes). The Hugging Face team gives a great overview of quantization and walks through the optimization steps for 8-bit matrix multiplication in transformers.\n",
        "\n",
        "Let's break down exactly what these arguments are doing.\n",
        "\n",
        "For documentation, please refer [here](https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig). We can use `BitsAndBytesConfig` to configure how we want to quantize model. In the code block below, the arguments specified accomplish the following:\n",
        "- `load_in_4bit` specifies that we want enable 4-bit quantization of the model.\n",
        "- `bnb_4bit_quant_type` sets the quantization data type in the bnb.nn.Linear4Bit layers to either \"nf4\" or \"fp4\".\n",
        "- `bnb_4bit_compute_dtype` sets the computation type the model will be working with, which can be different from the input type. Using \"float16\" can add aditional speed-ups to the fine-tuning process.\n",
        "- `use_nested_quant` makes quantization constants from the first quantization quantized again.\n",
        "\n",
        "Once the config file is all set up, we can pass it as an argument to `AutoModelForCausalLM.from_pretrained()`. Then, when we download LLaMa v2 from Hugging Face, 4-bit quantization is employed to compress the pretrained model!"
      ],
      "metadata": {
        "id": "ViUu_8VGZ_9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "iruRzPW6RmoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Base Model and Tokenizer"
      ],
      "metadata": {
        "id": "T5p_jotVRpR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned in the section above, we are using the `AutoModelForCausalLLM` to download a pretrained model on the Hugging Face Hub. You can utilize `Auto Classes` to automatically load the correct model architecture based on the name or path supplied. In this code, `AutoModelForCausalLM` is used to load a model with a causal language modeling head.\n",
        "\n",
        "For more documentation, refer [here](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM).\n",
        "\n",
        "In order to finetune the pretrained model, we need its tokenizer. Using the `AutoTokenizer`, this can be accomplished in a single line of code. The additional lines configure the tokenizer to be capatible with \"fp16\" training.  "
      ],
      "metadata": {
        "id": "OjReOt68HoIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the entire model on the GPU 0\n",
        "# We can use accelerate to infer a device map if we want to shard the model\n",
        "# between the GPU and CPU\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "2MqjOVmAHnhB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load base model using bitsandbytes config created above\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
      ],
      "metadata": {
        "id": "e3Ssi2IjRt68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have sucessfully loaded in a compressed version of LLaMa v2 with 4-bit quantization and its tokenizer!"
      ],
      "metadata": {
        "id": "qe8wgTzh12gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA Configuration"
      ],
      "metadata": {
        "id": "D2Ii1ZLaSr44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are almost ready to initialize our `SFTTrainer` to finetune LLaMa v2. So far, we have our quantized 4-bit LLM and its tokenizer. The final step involves setting up a `LoraConfig` instance that enables us to use LoRA to drastically reduce the number of trainable parameters in our LLM.\n",
        "\n",
        "At its core, LoRA uses the fact gradient updates for weight matrices in large neural networks can be effectively transcribed as low rank approximations. Aghajanyan et al. (2020) demonstrated that the models learned through over-parametrization actually exist within a significantly constrained dimensional space.\n",
        "\n",
        "LoRA is a fine-tuning method for large language models. It is not an algorithm that replaces the actual training process. In order to be effective, LoRA requires that you have a set of pre-trained weights. If these weights are garbage, then your fine-tuned model using LoRA will have poor performance as well.\n",
        "\n",
        "LoRA is predicated on the assumption that adaptations to the weight matrices,  when conforming to specific tasks, reside in low dimensional space relative to the entire matrix. Using this fact, LoRA stores weight updates as low rank matrice modules. During fine tuning, these modules (with far less training parameters) are updated through back propogation. When training is done, they are added to the frozen, quantized base model (in this case LLaMa v2).\n",
        "\n",
        "Below the arguments for the LoRA configuration are set. These will be used to create a `LoraConfig` instance for the \"peft_config\" argument for the `SFTTrainer`. For more information on what each argument is responsible for, please refer [here](https://huggingface.co/docs/peft/conceptual_guides/lora).\n",
        "\n",
        "Lets briefly break down the arguments:\n",
        "- `lora_alpha`: the LoRA scaling factor.\n",
        "- `lora_dropout`: the dropout probability of the LoRA layers.\n",
        "- `r`: the dimnesion of the LoRA modules that are added on top of the frozen weight matrices.\n",
        "- `bias`: Specifies if the bias parameters should be trained. None denotes none of the bias parameters will be trained.\n",
        "- `task_type`: the type of task the base model is responsible for."
      ],
      "metadata": {
        "id": "bipKMmSMFYXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "EiqeqC4LS33m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SFT Trainer Configuration"
      ],
      "metadata": {
        "id": "xH3NgjpBFnQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸš¨ Do not forget to modify the bf16 value when using A100's on Colab Pro!\n",
        "\n",
        "Here is what each argument specifies:\n",
        "- `output_dir`: Specifies the directory where the model predictions and checkpoints will be saved. E.g., \"./results\" will create or use the \"results\" folder in the current directory.\n",
        "\n",
        "- `num_train_epochs`: The total number of training epochs (complete passes through the training data). For example, a value of 1 means the model will be trained on the entire dataset once.\n",
        "\n",
        "- `per_device_train_batch_size`: Defines the batch size per GPU (or other hardware like TPU or CPU) for training. The total batch size will be this value multiplied by the number of devices used.\n",
        "\n",
        "- `gradient_accumulation_steps`: The number of update steps required to accumulate the gradients before performing a backward/update pass. It essentially splits a large batch into smaller ones to fit the GPU memory.\n",
        "\n",
        "- `optim`: The optimizer to be used during the training. Here, it is set to \"paged_adamw_32bit\", indicating a specific variant of the AdamW optimizer.\n",
        "\n",
        "- `save_steps`: Determines the number of update steps between two checkpoint saves. For instance, setting it to 25 means the model will be saved every 25 update steps.\n",
        "\n",
        "- `logging_steps`: Specifies how often to log training information such as loss, accuracy, etc. A value of 25 means logging every 25 update steps.\n",
        "\n",
        "- `learning_rate`: The initial learning rate for the AdamW optimizer. This controls how quickly or slowly the model learns from the data.\n",
        "\n",
        "- `weight_decay`: This represents the rate of decay applied to the weights during training, helping prevent overfitting by penalizing large weights.\n",
        "\n",
        "- `fp16`: A boolean indicating whether to use 16-bit (mixed) precision training (FP16) instead of 32-bit training. It can make training faster, with a slight trade-off in precision.\n",
        "\n",
        "- `bf16`: Similar to fp16 but uses brain floating-point 16-bit format (bf16). It can be set to True when training on specific hardware like NVIDIA's A100.\n",
        "\n",
        "- `max_grad_norm`: This parameter sets the maximum gradient norm for gradient clipping, preventing excessively large updates that can destabilize training.\n",
        "\n",
        "- `max_steps`: If set to a positive number, this defines the total number of training steps to perform, overriding num_train_epochs. A value of -1 means this is ignored.\n",
        "\n",
        "- `warmup_ratio`: Determines the portion of total training steps used for a linear warmup of the learning rate from 0 to its initial value. It can help the model start learning more gently.\n",
        "\n",
        "- `group_by_length`: When set to True, sequences of the same length are grouped together in batches. This can improve efficiency by minimizing the padding required.\n",
        "\n",
        "- `lr_scheduler_type`: Determines the type of learning rate scheduler to be used. In this case, it's set to \"constant\", meaning the learning rate does not change during training.\n",
        "\n",
        "- `report_to`: Defines where the training logs should be reported. Here, it's set to \"tensorboard\", so logs will be sent to TensorBoard for visualization.\n",
        "\n",
        "TrainerArgument library documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)."
      ],
      "metadata": {
        "id": "DqsLjR3uF5yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=25,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=True, # (set bf16 to True with an A100)\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"tensorboard\"\n",
        ")"
      ],
      "metadata": {
        "id": "7n8zcj_KFntA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the training arguments set up, we can now create a supervised fine-tuning trainer for LLaMa v2!\n",
        "\n",
        "If you are curious, you can find the Supervised Fine-tuning Trainer documentation [here](https://huggingface.co/docs/trl/main/en/sft_trainer)"
      ],
      "metadata": {
        "id": "E3G6E7xOGakR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False"
      ],
      "metadata": {
        "id": "elMp4PZ6GcId"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config, # Here's where the LoRA config is used\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments, # Here's where the training_arguments are used\n",
        "    packing=packing,\n",
        ")\n",
        "# Note since the quantized version of the model was loaded above, in order to\n",
        "# use the QLoRA algorithm all that is needed is to specify it in the peft_config\n",
        "# argument."
      ],
      "metadata": {
        "id": "WqtZPXM0Ga9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune and Save Trained LoRA Modules"
      ],
      "metadata": {
        "id": "AVF_caDEZhA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the `SFTTrainer` all set up, we are ready to begin finetuning!"
      ],
      "metadata": {
        "id": "mLzcuERC3Pwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I wrap the trainer in a simple helper function so we are able to track carbon emissions! The results will be uploaded to the current directory in an emissions.csv file. You can change the output diirecty location by specifying it as an argument. [Here](https://mlco2.github.io/codecarbon/installation.html) is the documentation."
      ],
      "metadata": {
        "id": "lMhLnTjJYiQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from codecarbon import track_emissions\n",
        "\n",
        "@track_emissions()\n",
        "def trainer_with_carbon_tracker(trainer):\n",
        "  # Train model\n",
        "  trainer.train()\n",
        "\n",
        "  # Save trained model\n",
        "  trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "dtJVoFuBOJhZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and save model with emissions tracker enabled\n",
        "trainer_with_carbon_tracker(trainer)"
      ],
      "metadata": {
        "id": "EhHONgkOZ1vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge LoRA Modules to Base Model and Save"
      ],
      "metadata": {
        "id": "vbJdu6nFbY9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once done training, we download the base model again and merge the LoRA modules with its weights. With this final step complete, the finetuned model is ready to be uploaded to the Hugging Face Hub ðŸ¤—"
      ],
      "metadata": {
        "id": "hrt1NDU03YPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "v1jiFphZbekX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ],
      "metadata": {
        "id": "dvlwiu7VblV1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}